BOW (Bag of Words): 

- 단어 가방을 만들고 분석할 텍스트에서 준비해둔 단어들이 나올때마다 count한다. (가방에 숫자를 채운다.) 
;따라서 단어의 수만큼의 가방이 필요하다.

- 특정 단어가 나온 횟수를 TF(Term Frequency), 특정 단어가 등장한 문서의 수를 DF(Document Frequency)라고 한다.

- DF가 높다는 것은 여러 곳에서 쓰이는 범용적인 단어라는 것을 의미하며, 반대로 특이하고 희귀한 단어를 IDF(Inverse DF)로 표현한다.

- TF-IDF는 TF x IDF로, 이 값이 높을 수록 타 문서에는 적게 등장하지만 이 문서에서는 많이 나온다는 것이다. 
; 즉, 이 문서의 핵심 키워드라는 것을 암시한다.

- BOW를 문서별로 다 구해서 하나의 data sheet로 만든 것을 DTM(Document-Term Matrix)라고 한다. 
; 문서들 이름이 Index, 가방들이 Column이 된다.



N-gram:

- BOW를 위처럼 하나의 단어 단위가 아닌 여러 단어의 묶음 단위로 작업할 수 있다.
; 예를 들어, Let's study 이 두 단어의 묶음이 자주 쓰인다면, 이 두 단어를 한 가방 안에 넣어둔다. 
; 이를 2-gram이라고 표현한다.



Distribution Hypothesis:

- BOW는 긍부정의 감정이나 문맥을 파악할 수 없다는 단점이 있다. 
; DH는 co-occurence를 이용하여 이를 해결한다.

- 한 단어는 주로 자주 같이 쓰이는 단어 cluster가 있다. 
; 이 경향을 바탕으로 문장이 하나의 cluster를 구성하고 있을 떄, 빈칸에 올 단어의 성향을 예측해볼 수 있다.

- 예를 들어, The (   ) sat on the mat.이라는 문장을 보면, 보통 괄호 안에는 cat, dog 등의 동물이 들어간다. 
; 그렇다면 학습할 때, 이런 동물들의 유사도가 높다고 학습한다.

- 텍스트를 여러 cluster의 구성이라고 봤을 때, 메인 topic을 뽑아내는 것도 가능하다.



벡터 유사도: 문장을 벡터화하여 나타낸다. ex. (2, 1, 0, 3)

- Euclidean Distance: 
; L1: 각 문장 간 성분의 차이의 절대값을 다 합한다.
; L2: 각 문장 간 성분의 차이를 제곱하고 더한 뒤 제곱근을 구한다.


- Cosine Distance:
; Euclidean처럼 두 포인트 사이의 거리를 구하는 것이 아니라, 각도의 크기를 이용한다.


- Infinity: 두 벡터의 성분 간 차이 중 가장 큰 값을 이용한다.



단어 기반 유사도

- Jaccard Similarity: 두 집합 간 공통된 요소의 비율, 즉 얼마나 겹치는지를 이용한다. 
; "겹치는 가방 수" / "두 문서의 전체 가방 수(중복은 한 번만 카운트)"



데이터셋 오픈소스:
- KLUE
- AI hub

NLP 모델 다운로드:
- Hugging Face



Annotation (= Labeling, 예를 들어, 이 이미지가 개인지 고양이인지 라벨링)을 쉽게 할 수 있게 도와주는 툴: https://labelstud.io
; 생성된 코드 수정해서 사용.
; 이미지의 어느 부분이 비행기이고 어느 부분이 차량인지 부분 별로 선택해서 라벨링도 가능.
; 텍스트부터 이미지, 음향, 비디오 등의 라벨링도 가능하며 다양한 기능이 있음.



Word Embedding (Encoding, Vectorization) = Text를 Word2Vec이나 FastText를 이용하여 벡터화 하는 작업.

1. Sparse Embedding

- One hot encoding 방식. 
; ex) 남자 [1 0 0], 여자 [0 1 0], 동물 [0 0 1]

- Word pool size에 따라 Vector size가 커지므로, 차원이 너무 커지는 경향이 있음.

- 크기와 반대로 대부분의 데이터가 0인 문제도 있음.

- 벡터가 Sparse해서 단어 의미를 벡터 공간에 표현할 수 없음.
; 단순히 단어를 벡터화 하는 것까지 밖에 안 됨.
; 문맥을 고려할 수 없고, 단어의 의미를 유추하는 것이 불가능.


2. Word2Vec

- 단어 A, B가 무슨 뜻인지 모르지만, 주변 단어의 문맥이 이전과 비슷하다면, 비슷한 단어 임을 유추할 수 있음.
; 그런 단어들은 가까이에 임베딩함.
; 단어의 의미를 고려하여 벡터화하기 때문에, 단어 의미를 유추할 수 있음.

- 벡터 연산도 가능함. 
; ex) "남 -> 여" 관계를 학습했다면, 삼촌을 넣으면 이모가 나오고, 왕을 넣으면 여왕이 나옴.

- 비지도학습 중 Self Supervised Learning에 해당하며, 데이터셋을 스스로 생성하여 학습함.

- Labeling 방법 1: CBOW
; I eat apple에서 I, apple을 주면 빈칸인 eat을 맞춤.
; eat, apple이면 I / I, apple이면 eat / I, eat이면 apple로 데이터셋을 스스로 만들어 학습.

- Labeling 방법 2: Skip N-gram (CBOW보다 우수한 성능)
; 역으로 단어 하나를 주면, 주변 단어를 맞춤.
; Skip 2-gram이면, 단어 하나를 주면, 양 옆으로 2개씩의 단어, 총 4개 단어를 맞춤.

- Word2Vec은 단어간 유사도나 관계 파악에 좋으며, 벡터 연산으로 추론 또한 가능함이 특징이다.
; 하지만, 단어의 Subword Inforamtion을 고려할 수 없다는 단점이 있다.
; 서울, 서울시, 서울시청을 학습했다 하더라도, 진천시청이 오면 그게 뭔지 모른다.
; 시청의 의미를 알았다면, 진천이라는 곳의 시청임을 예상할 수 있지만, 시청을 따로 학습하지 않아 발생하는 문제.
; 이를 OoV(Out of Vocabulary)라고 한다.


3. FastText

- OoV 문제를 해결하기 위해 만들어진 방법으로, 단어를 n-gram으로 분해해서 학습한다(n=2~5.)

- "<apple>"을 학습한다고 가정하자.
; 2-gram -> <a / ap / pp / pl / le / e>
; 3-gram -> <ap / app / ppl / ple / le>
; 4-gram -> <app / appl / pple / ple>
; 5-gram -> <appl / apple / pple>

- <apple>에 subword인 s가 추가되어 <apples>가 되더라도, 전체 데이터셋 중 s의 영향은 극히 일부가 된다.
; 2-gram -> <a / ap / pp / pl / le / es / s>
; 3-gram -> <ap / app / ppl / ple / les / es>
; 4-gram -> <app / appl / pple / ples / les>
; 5-gram -> <appl / apple / pples / ples>
; apple과 비슷한 부분은 여전히 너무나도 많으니, 유추를 쉽게 할 수 있다.


4. Subword Tokenization (Package: Sentencepiece, Huggingface)

- OoV 문제를 해결하기 위한 FastText 이외의 방법이다.

- Solution 1: BPE(Byte Pair Encoding) 학습
; 1. 문장을 띄어쓰기 기준으로 split한다.
; 2. 각 단어들을 글자 단위로 split한다.
; 3. 각 단어들의 첫 글자를 제외한 나머지 글자는 앞에 ##을 붙여준다(그냥 표시법으로, 다르게도 가능함.)
; 4. Bi-gram Pairs로 만든다.
; ex) 나는 너를 사랑해 -> 나, ##는, 너, ##를, 사, ##랑, ##해 -> (나, 는), (는, 너), (너, 를), (를, 사), (사, 랑), (랑, 해)
; Pair 중 가장 많이 출현한 쌍의 두 글자를 merge 하여 한 글자로 취급한다.
; (사, 랑)이 가장 빈도수가 높다면, '사랑'은 한 글자로 취급하여 다시 Bi-gram Pairs를 한다.
; (나, 는), (는, 너), (너, 를), (를, 사랑), (사랑, 해)