Activation Function:

- For Output Layer.: Identity Function(Regression), Softmax Function(Classification)

- For Hidden Layer.: ReLU, Sigmoid, tanh, etc.



Loss Function(Cost Function):

- MSE(Mean Squared Error).

- Cross Entropy.



Gradient Descent Code Example:

	from common.functions import softmax, cross_entropy_error
	from common.gradient import numerical_gradient

	class simpleNet():

		def __init__(self):
			self.W = np.random.randn(2, 3)

		def predict(self, x):
			return np.dot(x, self.W)

		def loss(self, x, t):
			z = self.predic(x)
			y = softmax(z)
			loss = cross_entropy_error(y, t)
		
			return loss

	x = np.array([0.6, 0.9])
	t = np.array([0, 0, 1])

	net = simpleNet()

	f = lambda w: net.loss(x, t)    # Print "net.loss(x, t) using the given "w".
	dW = nemerical_gradient(f, net.W)
	print(dW)



Stochastic Gradient Descent:

- In traditional Gradient Descent, the model parameters are updated based on the average gradient of the loss function computed over the entire dataset. However, in SGD, the parameters are updated after each training example or a small batch of examples. 
; This means that instead of computing the gradient of the loss function using the entire dataset, SGD computes the gradient using only one training example (or a small batch) at a time.



2 Layers Neural Network(Deep Learning) Code Example:

	Class TwoLayerNet:
	
		def __init__(self, input_size, hidden_size, output_size, weight_init_std = 0.01)    # Select weights in normal distribution
			self.params = {}
			self.params['W1'] = weight_init_std * np.random.randn(input_size, hidden_size)
			self.params['b1'] = np.zeros(hidden_size)
			self.params['W2'] = weight_init_std * np,random.randn(hidden_size, output_size)
			self.params['b2'] = np.zeros(output_size)

		def predict(self, x):
			W1, W2 = self.params['W1'], self.params['W2']
			b1, b2 = self.params['b1'], self.params['b2']
			
			a1 = np.dot(x, W1) + b1
			z1 = sigmoid(a1)
			a2 = np.dot(z1, W2) + b2
			y = softmax(a2)

			return y

		def loss(self, x, t):
			y = self.predict(x)

			return cross_entropy_error(y, t)

		def accuracy(self, x, t):
			y = self.predict(x)
			y = np.argmax(y, axis = 1)
			t = np.argmax(t, axis = 1)

			accuracy = np.sum(y == t) / float(x, shape[0])

			return accuracy

		def gradient(self, x, t):    # Using backpropagation for dW.
			W1, W2 = self.params['W1'], self.params['W2']
			b1, b2 = self.params['b1'], self.params['b2']
			grads = {}

			batch_num = x.shape[0]

			# Forward
			a1 = np.dot(x, W1) + b1
			z1 = sigmoid(a1)
			a2 = np.dox(z1, W2) + b2
			y = softmax(a2)
		
			# Backward
			dy = (y - t) / batch_num
			grads['W2'] = np.dot(z1.T, dy)
			grads['b2'] = np.sum(dy, axis = 0)

			da1 = np.dot(dy, W2.T)
			dz1 = sigmoid_grad(a1) * da1
			grads['W1'] = np.dot(x.T, dz1)
			grads['b1'] = np.sum(dz1, axis=0)

			return grads

	%%time
	from dataset.mnist import load_mnist

	(x_train, t_train), (x_test, t_test) = load_mnist(normalize=True, one_hot_label=True)

	iters_num = 10000  # Iteration number.
	train_size = x_train.shape[0]
	batch_size = 100   # Size of the mini batch.
	learning_rate = 0.1

	train_loss_list = []

	network = TwoLayerNet(input_size=784, hidden_size=50, output_size=10)

	for i in range(iters_num):

	    batch_mask = np.random.choice(train_size, batch_size)
	    x_batch = x_train[batch_mask]
	    t_batch = t_train[batch_mask]
    
	    # BackPropagation.
	    grad = network.gradient(x_batch, t_batch)
    
	    # Revising the hyperparameters.
	    for key in ('W1', 'b1', 'W2', 'b2'):
	        network.params[key] -= learning_rate * grad[key]
    
	    # Record the Learning.
	    loss = network.loss(x_batch, t_batch)
	    train_loss_list.append(loss)

	    # Calculate the accuracy by 1 epoch.
	    if i % iter_per_epoch == 0:
	        train_acc = network.accuracy(x_train, t_train)
	        test_acc = network.accuracy(x_test, t_test)
	        train_acc_list.append(train_acc)
	        test_acc_list.append(test_acc)
	        print(f'<{int(i//iter_per_epoch):03}> train acc: {train_acc:.3f}, test acc: {test_acc:.3f}')
	
	# Accuracy by Epochs
	markers = {'train': 'o', 'test': 's'}
	x = np.arange(len(train_acc_list))
	plt.plot(x, train_acc_list, label='train acc')
	plt.plot(x, test_acc_list, label='test acc', linestyle='--')
	plt.xlabel("epochs")
	plt.ylabel("accuracy")
	plt.ylim(0, 1.0)
	plt.legend(loc='lower right')
	plt.show()

	# Loss Function	
	plt.plot(train_loss_list)
	plt.title('손실 함수 값의 추이 (iter:10,000)')
	plt.xlabel('iteration')
	plt.ylabel('loss')
	plt.show()