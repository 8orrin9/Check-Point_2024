{"cells":[{"cell_type":"markdown","id":"f46fba2a","metadata":{"id":"f46fba2a"},"source":["# 정리"]},{"cell_type":"markdown","id":"5daf8f53","metadata":{"id":"5daf8f53"},"source":["- 기계학습에서 사용하는 데이터셋은 훈련 데이터와 시험 데이터로 나눠 사용한다.\n","- 훈련 데이터로 학습한모델의 범용능력을 시험 데이터로 평가한다.\n","- 신경망 학습은 손실 함수를 지표로, 손실 함수의 값이 작아지는 방항으로 가중치 매개변수를 갱신한다.\n","- 가중치 매개변수를 갱신할 때는 가중치 매개변수의 기울기를 이용하고, 기울어진 방향으로 가중치의 값을 갱신히는 작업을 반복한다.\n","- 아주 작은 값을 주었을 때의 차분으로 미분하는 것을 수치 미분이라고 한다.\n","- 수치 미분을 이용해 가중치 매개변수의 기울기를 구할수 있다.\n","- 수치 미분을 이용한 계산에는 시간이 걸리지만, 그 구현은 간단하다.\n","- 오차역전파법은 복잡하지만 기울기를 고속으로 구할수 있다."]},{"cell_type":"code","execution_count":null,"id":"fd942b0b","metadata":{"id":"fd942b0b"},"outputs":[],"source":[]},{"cell_type":"markdown","id":"f076fa13","metadata":{"id":"f076fa13"},"source":["---"]},{"cell_type":"markdown","id":"5d13de2c","metadata":{"id":"5d13de2c"},"source":["# NOTE  \n","  \n","Features(Columns)을 우리가 직접 setting하는 ML과 달리, DL은 알아서 setting하고 결과값까지 낸다.  \n","Output Layer에서 어떤 node는 코, 어떤 node는 귀를 의미하는 것이 이 때문이다.  \n","Feature를 설정해주지 않아도 알아서 한다.  \n","이를 Feature Engineering을 스스로 한다고 표현한다.  \n","  \n","<img src=\"./images/fig_4-2.png\" width=\"500\"/>  \n","    \n","---  \n","  \n","- 손실 함수 Cost Function  \n","  \n","도출된 결과값인 y와 정답인 t 간의 차이를 잔차라고 한다.  \n","모든 데이터에 대해 도출된 잔차의 합을 손실함수라고 부른다.    \n","이 잔차의 합인 Cost Function이 가장 작아지는 w 값들과 b 값을 구하는 것이 우리의 목표이다.  \n","  \n","Regression에서의 Cost Function을 MSE, Classification에서의 Cost Function을 Logross(Cross Entropy)라고 부른다.  \n","\n","MSE를 기준으로 예시를 들어보면..  \n","<img src=\"./images/e_4.1.png\" width=\"200\"/>  \n","위 식에서 y값은 w, x, b로 바꿔 표현할 수 있고, 그리하면 이 중 x, t는 알고 있으니, 결국 w, b만 미지수인 식이 된다.  \n","가장 최소가 되는 w, b 조합은 각각 Cost Function을 w로 편미분 한 것이 0, b로 편미분 한 것이 0이 되면 된다.  \n","\n","모든 데이터에 대해 위 작업을 하면.. w, b에 관한 연립방정식이 데이터 수만큼 생성되게 된다.  \n","너무 과도하게 많다. 그래서 나온 것이 경사하강법이다.  \n","\n","- 경사하강법  \n","\n","w축 (x축) - C축 (y축)의 좌표계에서, 임의의 w값일 때, C값이 최소값인지를 물어본다.  \n","즉, w 값에서 Cost Function의 미분 값이 0인지를 확인한다.  \n","아니라면 경사가 더 작은 곳으로 w를 한 칸 옮긴다.  \n","\n","이를 반복하며 최소의 C값이 나오는 곳을 찾는다.  \n","\n","그러나 한계점이 많다. 예를 들어, w자형 그래프의 경우 fake 최소점이 존재한다.  \n","이를 Local Minimum이라고 부른다.  \n","\n","이것을 극복하기 위한 특별한 경사하강법이 있는데, 그것을 Optimizer라고 부른다.(추후)  \n","\n","Classification의 경우, Cost Function만 로그로스로 바꿔서 위와 동일하게 작업하면 된다.\n","<img src=\"./images/e_4.2.png\" width=\"200\"/>\n","참고로 위 식에서 바깥에 시그마가 하나 더 있는게 옳은 식이다.  \n","위 식에서 k는 도출될 수 있는 Class의 갯수를 의미하고, 우리의 Cost Function은 모든 데이터에서 나온 잔차들을 합하여야 하므로..  \n","바깥에 n에 대한 시그마를 하나 더 넣어주고, n은 데이터의 수를 의미하게 하는 게 맞다.  \n","\n","---\n","\n","- 최대우도법  \n","\n","사실 Regression의 MSE와 Classification의 Logross는 모두 최대우도법이라는 같은 식을 변형한 것 뿐이다.  \n","Regression은 데이터 꼴이 정규 분포, Classification은 데이터 꼴이 베르누이 분포이기 때문이다.  \n","\n","---\n","\n","- 미니배치  \n","\n","Gradient Descent: 다음 스텝을 정할 때, 모든 경우의 수를 비교한다.  \n","Stochastic GD: 랜덤한 한 포인트나, 미니 배치만을 비교하여 다음 스텝을 정한다. 더 빠른 수렴이 가능하지만, 스무스하게 수렴하지는 않는다.  \n","\n","전체 데이터에서 n개를 샘플링하여 Cost Function을 구하고, w, b를 도출한다.  \n","복원 추출로써 다시 n개를 샘플링하여 Cost Function을 구하되, 경사하강법을 진행할 때, 최초로 집어넣는 초기값 w를 위에서 뽑은 w로 한다.  \n","\n","초기값을 바로 전 배치에서 뽑은 것으로 하는 것은, Local Minimum을 회피할 수 있는 여지를 만드는 중요한 요인이라고 한다.  \n","(Optimizer 중 하나.)  \n","\n","전체 데이터가 1000개, 1회 샘플링당 추출 샘플 수가 100개라고 할 때, 복원추출임에도 10번 배치를 돌리면 한 바퀴 다 돌았다고 본다.  \n","이것을 1 epoch 돌렸다고 표현한다.  \n","\n","경사하강법을 Gradient라고 한다면, 위 방식은 Stochastic GD라고 SGD라 부른다.  \n","\n","- Step_num  \n","\n","'step_num = 100'은 결과값 도출 과정을 100번 하라는 것을 의미한다.  \n","\n","예를 들어, mini batch의 크기가 100이고, total data가 1000개인 경우, 1 epoch은 10회이다.  \n","즉, step_num = 100은 10 epoch이다.  \n","\n","적절히 설정하지 않으면 너무 대충 결과가 나오기도, 아예 발산하여 결과를 낼 수 없는 경우도 있다.  \n","\n","---\n","\n","- Optimizer: DL의 패키지 중 하나로, (강화된) 경사하강법들을 통칭하는 용어이다.  \n","\n","1. Learning rate (Optimizer라기 보다는 그냥 Hyper Parameter에 가깝다.)    \n","한 칸씩 옮기면 너무 오래걸리니까 5 단위로 w를 옮겨가며 최소 C값을 찾는다.  \n","\n","2. Momentum  \n","Learning Rate는 고정이되, 이전 기울기에 가중치를 줘서 현재 기울기를 구한다.  \n","빠른 수렴에 집중한 알고리즘이다.  \n","\n","3. Adaptive Optimization Algorithm   \n","Momemtum과 유사하지만 효율성에 더 집중한 알고리즘이다.  \n","이것은, Learning Rate를 크게해도 될 상황에서는 크게, 작게 해야할 상황에서는 작게. 변경시켜가며 최소 C값을 찾는 방법이다.  \n","(기울기가 크면 기울기 0이 되는데 한참 걸릴테니 많이 이동하고, 작으면 곧 0이 될테니 소심하게 이동하는 방식.)  \n","\n","4. GD  \n","5. Mini Batch  \n","6. SGD  \n","7. Adam (Adaptive + Momentum)   \n","8. ...\n","\n","---  \n","기울기의 크기가 작은 방향으로 향한다.  \n","<img src=\"./images/e_4.7.png\" width=\"200\"/>  \n","\n","- $\\eta$: 학습율(learning rate)  \n","보통 \"Leraning rate x 기울기\" 씩 움직인다.  \n","\n","다변수함수의 기울기: Gradient = (dc/dw1. dc/dw2)  \n","기울기도 벡터형태로 표현이 가능하다.  \n","(일반적 함수는 Scalar로 결과가 나온다.)  \n","\n"]},{"cell_type":"markdown","id":"e7ceee41","metadata":{"id":"e7ceee41"},"source":["## Normal Two Layer"]},{"cell_type":"code","execution_count":null,"id":"09e65522","metadata":{"id":"09e65522"},"outputs":[],"source":["from common.functions import *\n","from common.gradient import numerical_gradient\n","\n","class TwoLayerNet:\n","\n","    def __init__(self, input_size, hidden_size, output_size, weight_init_std=0.01):\n","        # 가중치 초기화\n","        self.params = {}\n","        self.params['W1'] = weight_init_std * np.random.randn(input_size, hidden_size) #평균이 0, 표준편차가 0.1인 분포에서 랜덤하게 뽑을게~\n","        self.params['b1'] = np.zeros(hidden_size)\n","        self.params['W2'] = weight_init_std * np.random.randn(hidden_size, output_size)\n","        self.params['b2'] = np.zeros(output_size)\n","\n","    def predict(self, x):\n","        W1, W2 = self.params['W1'], self.params['W2']\n","        b1, b2 = self.params['b1'], self.params['b2']\n","\n","        a1 = np.dot(x, W1) + b1\n","        z1 = sigmoid(a1)\n","        a2 = np.dot(z1, W2) + b2\n","        y = softmax(a2)\n","\n","        return y\n","\n","    # x : 입력 데이터, t : 정답 레이블\n","    def loss(self, x, t):\n","        y = self.predict(x)\n","\n","        return cross_entropy_error(y, t)\n","\n","    def accuracy(self, x, t):\n","        y = self.predict(x)\n","        y = np.argmax(y, axis=1)\n","        t = np.argmax(t, axis=1)\n","\n","        accuracy = np.sum(y == t) / float(x.shape[0])\n","        return accuracy\n","\n","    # x : 입력 데이터, t : 정답 레이블\n","    def numerical_gradient(self, x, t): #numerical gradient 이용해서 기울기 구하기\n","        loss_W = lambda W: self.loss(x, t)\n","\n","        grads = {}\n","        grads['W1'] = numerical_gradient(loss_W, self.params['W1'])\n","        grads['b1'] = numerical_gradient(loss_W, self.params['b1'])\n","        grads['W2'] = numerical_gradient(loss_W, self.params['W2'])\n","        grads['b2'] = numerical_gradient(loss_W, self.params['b2'])\n","\n","        return grads\n","\n","    # 오차 역전파 이용(경사하강법 말고) 해서 기울기를 구한다.(추후 자세히)\n","    def gradient(self, x, t):\n","        W1, W2 = self.params['W1'], self.params['W2']\n","        b1, b2 = self.params['b1'], self.params['b2']\n","        grads = {}\n","\n","        batch_num = x.shape[0]\n","\n","        # forward\n","        a1 = np.dot(x, W1) + b1\n","        z1 = sigmoid(a1)\n","        a2 = np.dot(z1, W2) + b2\n","        y = softmax(a2)\n","\n","        # backward\n","        dy = (y - t) / batch_num\n","        grads['W2'] = np.dot(z1.T, dy)\n","        grads['b2'] = np.sum(dy, axis=0)\n","\n","        da1 = np.dot(dy, W2.T)\n","        dz1 = sigmoid_grad(a1) * da1\n","        grads['W1'] = np.dot(x.T, dz1)\n","        grads['b1'] = np.sum(dz1, axis=0)\n","\n","        return grads"]},{"cell_type":"markdown","id":"a616f48d","metadata":{"id":"a616f48d"},"source":["## Mini-batch"]},{"cell_type":"code","execution_count":null,"id":"8d8dc1df","metadata":{"id":"8d8dc1df","outputId":"65a41911-fd5e-49ec-f3cf-0d6bfd9cf926"},"outputs":[{"name":"stdout","output_type":"stream","text":["CPU times: total: 9.83 s\n","Wall time: 23.7 s\n"]}],"source":["%%time\n","from dataset.mnist import load_mnist\n","\n","# 데이터 읽기\n","(x_train, t_train), (x_test, t_test) = load_mnist(normalize=True, one_hot_label=True)\n","\n","# 하이퍼파라미터\n","iters_num = 10000  # 반복 횟수를 적절히 설정한다.\n","train_size = x_train.shape[0]\n","batch_size = 100   # 미니배치 크기\n","learning_rate = 0.1\n","\n","train_loss_list = []\n","\n","network = TwoLayerNet(input_size=784, hidden_size=50, output_size=10)\n","\n","for i in range(iters_num):\n","    # 미니배치 획득\n","    batch_mask = np.random.choice(train_size, batch_size)\n","    x_batch = x_train[batch_mask]\n","    t_batch = t_train[batch_mask]\n","\n","    # 기울기 계산\n","    #grad = network.numerical_gradient(x_batch, t_batch)\n","    # 오차 역전파 사용: 학습 시간 차이\n","    grad = network.gradient(x_batch, t_batch)\n","\n","    # 매개변수 갱신\n","    for key in ('W1', 'b1', 'W2', 'b2'):\n","        network.params[key] -= learning_rate * grad[key]\n","\n","    # 학습 경과 기록\n","    loss = network.loss(x_batch, t_batch)\n","    train_loss_list.append(loss)"]},{"cell_type":"code","execution_count":null,"id":"810a4484","metadata":{"id":"810a4484"},"outputs":[],"source":["# End of file"]},{"cell_type":"code","execution_count":null,"id":"85afea49","metadata":{"id":"85afea49"},"outputs":[],"source":[]}],"metadata":{"kernelspec":{"display_name":"Python 3 (ipykernel)","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.10.9"},"vp":{"vp_config_version":"1.0.0","vp_menu_width":273,"vp_note_display":true,"vp_note_width":263,"vp_position":{"width":541},"vp_section_display":false,"vp_signature":"VisualPython"},"colab":{"provenance":[],"collapsed_sections":["f46fba2a","e7ceee41"]}},"nbformat":4,"nbformat_minor":5}