BOW (Bag of Words): 
단어 가방을 만들고 분석할 텍스트에서 준비해둔 단어들이 나올때마다 count한다. (가방에 숫자를 채운다.) 따라서 단어의 수만큼의 가방이 필요하다.

특정 단어가 나온 횟수를 TF(Term Frequency), 특정 단어가 등장한 문서의 수를 DF(Document Frequency)라고 한다.

DF가 높다는 것은 여러 곳에서 쓰이는 범용적인 단어라는 것을 의미하며, 반대로 특이하고 희귀한 단어를 IDF(Inverse DF)로 표현한다.

TF-IDF는 TF x IDF로, 이 값이 높을 수록 타 문서에는 적게 등장하지만 이 문서에서는 많이 나온다는 것이다. 즉, 이 문서의 핵심 키워드라는 것을 암시한다.

BOW를 문서별로 다 구해서 하나의 data sheet로 만든 것을 DTM(Document-Term Matrix)라고 한다. 문서들 이름이 Index, 가방들이 Column이 된다.



N-gram:
BOW를 위처럼 하나의 단어 단위가 아닌 여러 단어의 묶음 단위로 작업할 수 있다.

예를 들어, Let's study 이 두 단어의 묶음이 자주 쓰인다면, 이 두 단어를 한 가방 안에 넣어둔다. 이를 2-gram이라고 표현한다.



Distribution Hypothesis:
BOW는 긍부정의 감정이나 문맥을 파악할 수 없다는 단점이 있다. DH는 co-occurence를 이용하여 이를 해결한다.

한 단어는 주로 자주 같이 쓰이는 단어 cluster가 있다. 이 경향을 바탕으로 문장이 하나의 cluster를 구성하고 있을 떄, 빈칸에 올 단어의 성향을 예측해볼 수 있다.

예를 들어, The (   ) sat on the mat.이라는 문장을 보면, 보통 괄호 안에는 cat, dog 등의 동물이 들어간다. 그렇다면 학습할 때, 이런 동물들의 유사도가 높다고 학습한다.

텍스트를 여러 cluster의 구성이라고 봤을 때, 메인 topic을 뽑아내는 것도 가능하다.



벡터 유사도: 문장을 벡터화하여 나타낸다. ex. (2, 1, 0, 3)

Euclidean Distance: 
L1: 각 문장 간 성분의 차이의 절대값을 다 합한다.
L2: 각 문장 간 성분의 차이를 제곱하고 더한 뒤 제곱근을 구한다.

Cosine Distance:
Euclidean처럼 두 포인트 사이의 거리를 구하는 것이 아니라, 각도의 크기를 이용한다.

Infinity: 두 벡터의 성분 간 차이 중 가장 큰 값을 이용한다.



단어 기반 유사도
Jaccard Similarity: 두 집합 간 공통된 요소의 비율, 즉 얼마나 겹치는지를 이용한다. 
"겹치는 가방 수" / "두 문서의 전체 가방 수(중복은 한 번만 카운트)"



데이터셋 오픈소스:
- KLUE
- AI hub