특성 추출(feature extraction) 및 특성 선택(feature selection)은 머신러닝 모델의 성능을 향상시키기 위해 데이터를 처리하는 중요한 단계입니다. 각각의 접근 방식은 특정 상황에 맞게 특성을 선택하거나 변형하여 모델의 성능을 개선하려고 합니다. 여기서는 **일변량 통계(Statistical Univariate Analysis)**, **모델 기반 선택(Model-Based Selection)**, **반복 선택(Recursive Feature Selection)**의 세 가지 접근 방식을 설명하겠습니다.

### 1. 일변량 통계 (Univariate Statistics)

**일변량 통계**는 각 특성이 독립적으로 타겟 변수와 얼마나 잘 연관되는지를 평가합니다. 일반적으로 통계적 테스트나 측정 지표를 사용하여 특성과 타겟 변수 간의 관계를 분석합니다.

#### 주요 방법:
- **카이 제곱 테스트 (Chi-Square Test)**: 주로 범주형 데이터에서 각 특성과 타겟 변수 간의 독립성을 평가합니다.
- **상관 계수 (Correlation Coefficient)**: 연속형 특성과 타겟 변수 간의 선형 관계를 측정합니다. 예를 들어, 피어슨 상관 계수를 사용합니다.
- **ANOVA (Analysis of Variance)**: 여러 그룹 간의 평균 차이를 비교하여 특성이 타겟 변수에 미치는 영향을 평가합니다.

#### 장점:
- 간단하고 이해하기 쉬운 방법입니다.
- 데이터의 각 특성을 개별적으로 평가할 수 있습니다.

#### 단점:
- 특성 간의 상호작용을 고려하지 않으며, 개별적인 특성만 분석합니다.

### 2. 모델 기반 선택 (Model-Based Selection)

**모델 기반 선택**은 특정 머신러닝 모델을 사용하여 특성의 중요성을 평가하고 선택하는 방법입니다. 모델의 성능을 기준으로 중요한 특성을 식별합니다.

#### 주요 방법:
- **L1 정규화 (Lasso Regression)**: Lasso 회귀는 특성의 계수를 0으로 만들 수 있는 정규화 방법입니다. 이를 통해 중요하지 않은 특성을 제거할 수 있습니다.
- **트리 기반 모델 (Tree-Based Models)**: 결정 트리, 랜덤 포레스트, 그래디언트 부스팅과 같은 모델은 특성의 중요도를 자연스럽게 평가합니다. 예를 들어, 랜덤 포레스트의 특성 중요도 메서드가 이에 해당합니다.
- **임베디드 방법 (Embedded Methods)**: 모델 학습 과정에서 특성 선택을 수행합니다. 예를 들어, L1 정규화를 사용하는 로지스틱 회귀는 임베디드 방법의 일종입니다.

#### 장점:
- 특성의 중요성을 모델 학습 과정에서 직접 평가하므로 데이터와 모델의 특성에 맞게 조정됩니다.
- 상호작용과 비선형 관계를 처리할 수 있습니다.

#### 단점:
- 특정 모델에 의존적이며, 모델이 잘못 선택되면 성능이 저하될 수 있습니다.

### 3. 반복 선택 (Recursive Feature Selection, RFE)

**반복 선택**은 특성을 반복적으로 선택하고 제거하여 모델 성능을 최적화하는 방법입니다. 이 방법은 특성의 중요도를 평가하는 모델을 사용하여 특성의 하위 집합을 선택합니다.

#### 주요 방법:
- **RFE (Recursive Feature Elimination)**: 기본 모델(예: SVM, 로지스틱 회귀)을 사용하여 특성의 중요도를 평가하고, 중요도가 낮은 특성을 제거하며, 이를 반복하여 최적의 특성 집합을 찾습니다.
- **RFECV (Recursive Feature Elimination with Cross-Validation)**: RFE에 교차 검증을 추가하여 특성 선택의 안정성을 평가하고 최적의 특성 개수를 결정합니다.

#### 장점:
- 모델의 성능을 기준으로 특성을 선택하므로 실제 모델의 성능 향상에 기여합니다.
- 비선형 관계와 상호작용을 처리할 수 있습니다.

#### 단점:
- 계산 비용이 많이 들 수 있으며, 데이터와 특성의 수가 많을 경우 시간이 오래 걸릴 수 있습니다.
- 선택 과정에서 모델의 학습이 여러 번 수행되므로 시간이 소요될 수 있습니다.

이러한 방법들은 각각의 장단점이 있으며, 데이터와 문제의 특성에 따라 적절한 방법을 선택하는 것이 중요합니다.
